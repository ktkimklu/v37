---
title: A Deeper Look at Planning as Learning from Replay
abstract: In reinforcement learning, the notions of experience replay, and of planning
  as learning from replayed experience, have long been used to find good policies
  with minimal training data. Replay can be seen either as model-based reinforcement
  learning, where the store of past experiences serves as the model, or as a way to
  avoid a conventional model of the environment altogether. In this paper, we look
  more deeply at how replay blurs the line between model-based and model-free methods.
  First, we show for the first time an exact equivalence between the sequence of value
  functions found by a model-based policy-evaluation method and by a model-free method
  with replay. Second, we present a general replay method that can mimic a spectrum
  of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based
  (linear Dyna). Finally, we use insights gained from these relationships to design
  a new model-based reinforcement learning algorithm for linear function approximation.
  This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda)
  because it extends more naturally to online control, and improves upon linear Dyna
  because it is a multi-step method, enabling it to perform well even in non-Markov
  problems or, equivalently, in problems with significant function approximation.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: vanseijen15
month: 0
firstpage: 2314
lastpage: 2322
page: 2314-2322
sections: 
author:
- given: Harm
  family: Vanseijen
- given: Rich
  family: Sutton
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/vanseijen15/vanseijen15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
