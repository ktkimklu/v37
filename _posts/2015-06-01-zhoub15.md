---
supplementary: Supplementary:zhoub15-supp.pdf
title: "\\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis
  of First-Order Methods"
abstract: Recently, \ell_1,p-regularization has been widely used to induce structured
  sparsity in the solutions to various optimization problems. Motivated by the desire
  to analyze the convergence rate of first-order methods, we show that for a large
  class of \ell_1,p-regularized problems, an error bound condition is satisfied when
  p∈[1,2] or p=∞but fails to hold for any p∈(2,∞). Based on this result, we show that
  many first-order methods enjoy an asymptotic linear rate of convergence when applied
  to \ell_1,p-regularized linear or logistic regression with p∈[1,2] or p=∞. By contrast,
  numerical experiments suggest that for the same class of problems with p∈(2,∞),
  the aforementioned methods may not converge linearly.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhoub15
month: 0
tex_title: "$\\ell_{1,p}$-Norm Regularization: Error Bounds and Convergence Rate Analysis
  of First-Order Methods"
firstpage: 1501
lastpage: 1510
page: 1501-1510
sections: 
author:
- given: Zirui
  family: Zhou
- given: Qi
  family: Zhang
- given: Anthony Man-Cho
  family: So
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/zhoub15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
