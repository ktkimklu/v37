---
supplementary: Supplementary:hu15-supp.pdf
title: Large-scale Distributed Dependent Nonparametric Trees
abstract: 'Practical applications of Bayesian nonparametric (BNP) models have been
  limited, due to their high computational complexity and poor scaling on large data.
  In this paper, we consider dependent nonparametric trees (DNTs), a powerful infinite
  model that captures time-evolving hierarchies, and develop a large-scale distributed
  training system. Our major contributions include: (1) an effective memoized variational
  inference for DNTs, with a novel birth-merge strategy for exploring the unbounded
  tree space; (2) a model-parallel scheme for concurrent tree growing/pruning and
  efficient model alignment, through conflict-free model partitioning and lightweight
  synchronization; (3) a data-parallel scheme for variational parameter updates that
  allows distributed processing of massive data. Using 64 cores in 36 hours, our system
  learns a 10K-node DNT topic model on 8M documents that captures both high-frequency
  and long-tail topics. Our data and model scales are orders-of-magnitude larger than
  recent results on the hierarchical Dirichlet process, and the near-linear scalability
  indicates great potential for even bigger problem sizes.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hu15
month: 0
firstpage: 1651
lastpage: 1659
page: 1651-1659
sections: 
author:
- given: Zhiting
  family: Hu
- given: Ho
  family: Qirong
- given: Avinava
  family: Dubey
- given: Eric
  family: Xing
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/hu15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
