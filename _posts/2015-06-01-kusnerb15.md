---
title: From Word Embeddings To Document Distances
abstract: We present the Word Mover’s Distance (WMD), a novel distance function between
  text documents. Our work is based on recent results in word embeddings that learn
  semantically meaningful representations for words from local co-occurrences in sentences.
  The WMD distance measures the dissimilarity between two text documents as the minimum
  amount of distance that the embedded words of one document need to "travel" to reach
  the embedded words of another document. We show that this distance metric can be
  cast as an instance of the Earth Mover’s Distance, a well studied transportation
  problem for which several highly efficient solvers have been developed. Our metric
  has no hyperparameters and is straight-forward to implement. Further, we demonstrate
  on eight real world document classification data sets, in comparison with seven
  state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest
  neighbor document classification error rates.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kusnerb15
month: 0
tex_title: From Word Embeddings To Document Distances
firstpage: 957
lastpage: 966
page: 957-966
sections: 
author:
- given: Matt
  family: Kusner
- given: Yu
  family: Sun
- given: Nicholas
  family: Kolkin
- given: Kilian
  family: Weinberger
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/kusnerb15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
