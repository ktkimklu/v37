---
supplementary: Supplementary:maclaurin15-supp.pdf
title: Gradient-based Hyperparameter Optimization through Reversible Learning
abstract: Tuning hyperparameters of learning algorithms is hard because gradients
  are usually unavailable. We compute exact gradients of cross-validation performance
  with respect to all hyperparameters by chaining derivatives backwards through the
  entire training procedure. These gradients allow us to optimize thousands of hyperparameters,
  including step-size and momentum schedules, weight initialization distributions,
  richly parameterized regularization schemes, and neural network architectures. We
  compute hyperparameter gradients by exactly reversing the dynamics of stochastic
  gradient descent with momentum.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: maclaurin15
month: 0
firstpage: 2113
lastpage: 2122
page: 2113-2122
sections: 
author:
- given: Dougal
  family: Maclaurin
- given: David
  family: Duvenaud
- given: Ryan
  family: Adams
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/maclaurin15/maclaurin15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
