---
title: Efficient Training of LDA on a GPU by Mean-for-Mode Estimation
abstract: We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs
  sampler that we use to train LDA on a GPU. The algorithm combines benefits of both
  uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler — and unlike
  an uncollapsed Gibbs sampler — it has good statistical performance, and can use
  sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed
  Gibbs sampler — and unlike a collapsed Gibbs sampler — it is embarrassingly parallel,
  and can use approximate counters.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: tristan15
month: 0
firstpage: 59
lastpage: 68
page: 59-68
sections: 
author:
- given: Jean-Baptiste
  family: Tristan
- given: Joseph
  family: Tassarotti
- given: Guy
  family: Steele
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/tristan15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
