---
title: Theory of Dual-sparse Regularized Randomized Reduction
abstract: In this paper, we study randomized reduction methods, which reduce high-dimensional
  features into low-dimensional space by randomized methods (e.g., random projection,
  random hashing), for large-scale high-dimensional classification. Previous theoretical
  results on randomized reduction methods hinge on strong assumptions about the data,
  e.g., low rank of the data matrix or a large separable margin of classification,
  which hinder their in broad domains. To address these limitations, we propose dual-sparse
  regularized randomized reduction methods that introduce a sparse regularizer into
  the reduced dual problem. Under a mild condition that the original dual solution
  is a (nearly) sparse vector, we show that the resulting dual solution is close to
  the original dual solution and concentrates on its support set. In numerical experiments,
  we present an empirical study to support the analysis and we also present a novel
  application of the dual-sparse randomized reduction methods to reducing the communication
  cost of distributed learning from large-scale high-dimensional data.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yangb15
month: 0
firstpage: 305
lastpage: 314
page: 305-314
sections: 
author:
- given: Tianbao
  family: Yang
- given: Lijun
  family: Zhang
- given: Rong
  family: Jin
- given: Shenghuo
  family: Zhu
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/yangb15/yangb15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
