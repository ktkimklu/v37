---
title: Learning Local Invariant Mahalanobis Distances
abstract: For many tasks and data types, there are natural transformations to which
  the data should be invariant or insensitive. For instance, in visual recognition,
  natural images should be insensitive to rotation and translation. This requirement
  and its implications have been important in many machine learning applications,
  and tolerance for image transformations was primarily achieved by using robust feature
  vectors. In this paper we propose a novel and computationally efficient way to learn
  a local Mahalanobis metric per datum, and show how we can learn a local invariant
  metric to any transformation in order to improve performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: fetaya15
month: 0
tex_title: Learning Local Invariant Mahalanobis Distances
firstpage: 162
lastpage: 168
page: 162-168
sections: 
author:
- given: Ethan
  family: Fetaya
- given: Shimon
  family: Ullman
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/fetaya15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
