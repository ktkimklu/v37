---
supplementary: Supplementary:jiang15-supp.pdf
title: Abstraction Selection in Model-based Reinforcement Learning
abstract: State abstractions are often used to reduce the complexity of model-based
  reinforcement learning when only limited quantities of data are available. However,
  choosing the appropriate level of abstraction is an important problem in practice.
  Existing approaches have theoretical guarantees only under strong assumptions on
  the domain or asymptotically large amounts of data, but in this paper we propose
  a simple algorithm based on statistical hypothesis testing that comes with a finite-sample
  guarantee under assumptions on candidate abstractions. Our algorithm trades off
  the low approximation error of finer abstractions against the low estimation error
  of coarser abstractions, resulting in a loss bound that depends only on the quality
  of the best available abstraction and is polynomial in planning horizon.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jiang15
month: 0
firstpage: 179
lastpage: 188
page: 179-188
sections: 
author:
- given: Nan
  family: Jiang
- given: Alex
  family: Kulesza
- given: Satinder
  family: Singh
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/jiang15/jiang15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
