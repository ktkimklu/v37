---
supplementary: Supplementary:perrot15-supp.pdf
title: A Theoretical Analysis of Metric Hypothesis Transfer Learning
abstract: We consider the problem of transferring some a priori knowledge in the context
  of supervised metric learning approaches. While this setting has been successfully
  applied in some empirical contexts, no theoretical evidence exists to justify this
  approach. In this paper, we provide a theoretical justification based on the notion
  of algorithmic stability adapted to the regularized metric learning setting. We
  propose an on-average-replace-two-stability model allowing us to prove fast generalization
  rates when an auxiliary source metric is used to bias the regularizer. Moreover,
  we prove a consistency result from which we show the interest of considering biased
  weighted regularized formulations and we provide a solution to estimate the associated
  weight. We also present some experiments illustrating the interest of the approach
  in standard metric learning tasks and in a transfer learning problem where few labelled
  data are available.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: perrot15
month: 0
firstpage: 1708
lastpage: 1717
page: 1708-1717
sections: 
author:
- given: MichaÃ«l
  family: Perrot
- given: Amaury
  family: Habrard
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/perrot15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
