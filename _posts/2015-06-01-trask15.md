---
title: Modeling Order in Neural Word Embeddings at Scale
abstract: Natural Language Processing (NLP) systems commonly leverage bag-of-words
  co-occurrence techniques to capture semantic and syntactic word relationships. The
  resulting word-level distributed representations often ignore morphological information,
  though character-level embeddings have proven valuable to NLP tasks. We propose
  a new neural language model incorporating both word order and character order in
  its embedding. The model produces several vector spaces with meaningful substructure,
  as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding
  best published syntactic word-analogy scores by a 58% error margin. Furthermore,
  the model includes several parallel training methods, most notably allowing a skip-gram
  network with 160 billion parameters to be trained overnight on 3 multi-core CPUs,
  14x larger than the previous largest neural network.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: trask15
month: 0
tex_title: Modeling Order in Neural Word Embeddings at Scale
firstpage: 2266
lastpage: 2275
page: 2266-2275
sections: 
author:
- given: Andrew
  family: Trask
- given: David
  family: Gilmore
- given: Matthew
  family: Russell
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/trask15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
