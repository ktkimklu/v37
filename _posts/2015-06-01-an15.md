---
title: How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?
abstract: This paper investigates how hidden layers of deep rectifier networks are
  capable of transforming two or more pattern sets to be linearly separable while
  preserving the distances with a guaranteed degree, and proves the universal classification
  power of such distance preserving rectifier networks. Through the nearly isometric
  nonlinear transformation in the hidden layers, the margin of the linear separating
  plane in the output layer and the margin of the nonlinear separating boundary in
  the original data space can be closely related so that the maximum margin classification
  in the input data space can be achieved approximately via the maximum margin linear
  classifiers in the output layer. The generalization performance of such distance
  preserving deep rectifier neural networks can be well justified by the distance-preserving
  properties of their hidden layers and the maximum margin property of the linear
  classifiers in the output layer.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: an15
month: 0
tex_title: How Can Deep Rectifier Networks Achieve Linear Separability and Preserve
  Distances?
firstpage: 514
lastpage: 523
page: 514-523
order: 514
cycles: false
author:
- given: Senjian
  family: An
- given: Farid
  family: Boussaid
- given: Mohammed
  family: Bennamoun
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/an15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
