---
title: Weight Uncertainty in Neural Network
abstract: We introduce a new, efficient, principled and backpropagation-compatible
  algorithm for learning a probability distribution on the weights of a neural network,
  called Bayes by Backprop. It regularises the weights by minimising a compression
  cost, known as the variational free energy or the expected lower bound on the marginal
  likelihood. We show that this principled kind of regularisation yields comparable
  performance to dropout on MNIST classification. We then demonstrate how the learnt
  uncertainty in the weights can be used to improve generalisation in non-linear regression
  problems, and how this weight uncertainty can be used to drive the exploration-exploitation
  trade-off in reinforcement learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: blundell15
month: 0
tex_title: Weight Uncertainty in Neural Network
firstpage: 1613
lastpage: 1622
page: 1613-1622
order: 1613
cycles: false
author:
- given: Charles
  family: Blundell
- given: Julien
  family: Cornebise
- given: Koray
  family: Kavukcuoglu
- given: Daan
  family: Wierstra
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/blundell15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
