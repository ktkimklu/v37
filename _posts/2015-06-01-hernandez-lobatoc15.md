---
supplementary: Supplementary:hernandez-lobatoc15-supp.pdf
title: Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks
abstract: Large multilayer neural networks trained with backpropagation have recently
  achieved state-of-the-art results in a wide range of problems. However, using backprop
  for neural net learning still has some disadvantages, e.g., having to tune a large
  number of hyperparameters to the data, lack of calibrated probabilistic predictions,
  and a tendency to overfit the training data. In principle, the Bayesian approach
  to learning neural networks does not have these problems. However, existing Bayesian
  techniques lack scalability to large dataset and network sizes. In this work we
  present a novel scalable method for learning Bayesian neural networks, called probabilistic
  backpropagation (PBP). Similar to classical backpropagation, PBP works by computing
  a forward propagation of probabilities through the network and then doing a backward
  computation of gradients. A series of experiments on ten real-world datasets show
  that PBP is significantly faster than other techniques, while offering competitive
  predictive abilities. Our experiments also show that PBP provides accurate estimates
  of the posterior variance on the network weights.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hernandez-lobatoc15
month: 0
firstpage: 1861
lastpage: 1869
page: 1861-1869
sections: 
author:
- given: Jose Miguel
  family: Hernandez-Lobato
- given: Ryan
  family: Adams
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/hernandez-lobatoc15/hernandez-lobatoc15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
