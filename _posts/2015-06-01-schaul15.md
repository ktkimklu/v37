---
supplementary: Supplementary:schaul15-supp.pdf
title: Universal Value Function Approximators
abstract: Value functions are a core component of reinforcement learning. The main
  idea is to to construct a single function approximator V(s; theta) that estimates
  the long-term reward from any state s, using parameters Î¸. In this paper we introduce
  universal value function approximators (UVFAs) V(s,g;theta) that generalise not
  just over states s but also over goals g. We develop an efficient technique for
  supervised learning of UVFAs, by factoring observed values into separate embedding
  vectors for state and goal, and then learning a mapping from s and g to these factored
  embedding vectors. We show how this technique may be incorporated into a reinforcement
  learning algorithm that updates the UVFA solely from observed rewards. Finally,
  we demonstrate that a UVFA can successfully generalise to previously unseen goals.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: schaul15
month: 0
firstpage: 1312
lastpage: 1320
page: 1312-1320
sections: 
author:
- given: Tom
  family: Schaul
- given: Daniel
  family: Horgan
- given: Karol
  family: Gregor
- given: David
  family: Silver
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/schaul15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
