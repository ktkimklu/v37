---
title: Distributed Gaussian Processes
abstract: To scale Gaussian processes (GPs) to large data sets we introduce the robust
  Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model
  for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations,
  the rBCM is conceptually simple and does not rely on inducing or variational parameters.
  The key idea is to recursively distribute computations to independent computational
  units and, subsequently, recombine them to form an overall result. Efficient closed-form
  inference allows for straightforward parallelisation and distributed computations
  with a small memory footprint. The rBCM is independent of the computational graph
  and can be used on heterogeneous computing infrastructures, ranging from laptops
  to clusters. With sufficient computing resources our distributed GP model can handle
  arbitrarily large data sets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: deisenroth15
month: 0
firstpage: 1481
lastpage: 1490
page: 1481-1490
sections: 
author:
- given: Marc
  family: Deisenroth
- given: Jun Wei
  family: Ng
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/deisenroth15/deisenroth15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
