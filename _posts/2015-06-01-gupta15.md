---
title: Deep Learning with Limited Numerical Precision
abstract: Training of large-scale deep neural networks is often constrained by the
  available computational resources. We study the effect of limited precision data
  representation and computation on neural network training. Within the context of
  low-precision fixed-point computations, we observe the rounding scheme to play a
  crucial role in determining the networkâ€™s behavior during training. Our results
  show that deep networks can be trained using only 16-bit wide fixed-point number
  representation when using stochastic rounding, and incur little to no degradation
  in the classification accuracy. We also demonstrate an energy-efficient hardware
  accelerator that implements low-precision fixed-point arithmetic with stochastic
  rounding
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gupta15
month: 0
firstpage: 1737
lastpage: 1746
page: 1737-1746
sections: 
author:
- given: Suyog
  family: Gupta
- given: Ankur
  family: Agrawal
- given: Kailash
  family: Gopalakrishnan
- given: Pritish
  family: Narayanan
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/gupta15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
