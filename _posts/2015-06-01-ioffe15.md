---
supplementary: Supplementary:ioffe15-supp.pdf
title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
  Covariate Shift'
abstract: 'Training Deep Neural Networks is complicated by the fact that the distribution
  of each layerâ€™s inputs changes during training, as the parameters of the previous
  layers change. This slows down the training by requiring lower learning rates and
  careful parameter initialization, and makes it notoriously hard to train models
  with saturating nonlinearities. We refer to this phenomenon as internal covariate
  shift, and address the problem by normalizing layer inputs. Our method draws its
  strength from making normalization a part of the model architecture and performing
  the normalization for each training mini-batch. Batch Normalization allows us to
  use much higher learning rates and be less careful about initialization, and in
  some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification
  model, Batch Normalization achieves the same accuracy with 14 times fewer training
  steps, and beats the original model by a significant margin. Using an ensemble of
  batch-normalized networks, we improve upon the best published result on ImageNet
  classification: reaching 4.82% top-5 test error, exceeding the accuracy of human
  raters.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ioffe15
month: 0
firstpage: 448
lastpage: 456
page: 448-456
sections: 
author:
- given: Sergey
  family: Ioffe
- given: Christian
  family: Szegedy
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/ioffe15/ioffe15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
