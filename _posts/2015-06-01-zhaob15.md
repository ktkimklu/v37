---
title: Adaptive Stochastic Alternating Direction Method of Multipliers
abstract: The Alternating Direction Method of Multipliers (ADMM) has been studied
  for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical)
  expected loss function on all training examples, resulting in a computational complexity
  proportional to the number of training examples. To reduce the complexity, stochastic
  ADMM algorithms were proposed to replace the expected loss function with a random
  loss function associated with one uniformly drawn example plus a Bregman divergence
  term. The Bregman divergence, however, is derived from a simple 2nd-order proximal
  function, i.e., the half squared norm, which could be a suboptimal choice. In this
  paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order
  proximal functions, which produce a new family of adaptive stochastic ADMM methods.
  We theoretically prove that the regret bounds are as good as the bounds which could
  be achieved by the best proximal function that can be chosen in hindsight. Encouraging
  empirical results on a variety of real-world datasets confirm the effectiveness
  and efficiency of the proposed algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhaob15
month: 0
tex_title: Adaptive Stochastic Alternating Direction Method of Multipliers
firstpage: 69
lastpage: 77
page: 69-77
sections: 
author:
- given: Peilin
  family: Zhao
- given: Jinwei
  family: Yang
- given: Tong
  family: Zhang
- given: Ping
  family: Li
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/zhaob15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
