---
supplementary: Supplementary:wangg15-supp.pdf
title: 'Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo'
abstract: We consider the problem of Bayesian learning on sensitive datasets and present
  two simple but somewhat surprising results that connect Bayesian learning to “differential
  privacy”, a cryptographic approach to protect individual-level privacy while permitting
  database-level utility. Specifically, we show that under standard assumptions, getting
  one sample from a posterior distribution is differentially private “for free”; and
  this sample as a statistical estimator is often consistent, near optimal, and computationally
  tractable. Similarly but separately, we show that a recent line of work that use
  stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially
  privacy with minor or no modifications of the algorithmic procedure at all, these
  observations lead to an “anytime” algorithm for Bayesian learning under privacy
  constraint. We demonstrate that it performs much better than the state-of-the-art
  differential private methods on synthetic and real datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wangg15
month: 0
firstpage: 2493
lastpage: 2502
page: 2493-2502
sections: 
author:
- given: Yu-Xiang
  family: Wang
- given: Stephen
  family: Fienberg
- given: Alex
  family: Smola
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/wangg15/wangg15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
