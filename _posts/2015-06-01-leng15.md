---
title: Hashing for Distributed Data
abstract: Recently, hashing based approximate nearest neighbors search has attracted
  much attention. Extensive centralized hashing algorithms have been proposed and
  achieved promising performance. However, due to the large scale of many applications,
  the data is often stored or even collected in a distributed manner. Learning hash
  functions by aggregating all the data into a fusion center is infeasible because
  of the prohibitively expensive communication and computation overhead. In this paper,
  we develop a novel hashing model to learn hash functions in a distributed setting.
  We cast a centralized hashing model as a set of subproblems with consensus constraints.
  We find these subproblems can be analytically solved in parallel on the distributed
  compute nodes. Since no training data is transmitted across the nodes in the learning
  process, the communication cost of our model is independent to the data size. Extensive
  experiments on several large scale datasets containing up to 100 million samples
  demonstrate the efficacy of our method.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: leng15
month: 0
tex_title: Hashing for Distributed Data
firstpage: 1642
lastpage: 1650
page: 1642-1650
sections: 
author:
- given: Cong
  family: Leng
- given: Jiaxiang
  family: Wu
- given: Jian
  family: Cheng
- given: Xi
  family: Zhang
- given: Hanqing
  family: Lu
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/leng15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
