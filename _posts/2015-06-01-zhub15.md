---
title: Long Short-Term Memory Over Recursive Structures
abstract: The chain-structured long short-term memory (LSTM) has showed to be effective
  in a wide range of problems such as speech recognition and machine translation.
  In this paper, we propose to extend it to tree structures, in which a memory cell
  can reflect the history memories of multiple child cells or multiple descendant
  cells in a recursive process. We call the model S-LSTM, which provides a principled
  way of considering long-distance interaction over hierarchies, e.g., language or
  image parse structures. We leverage the models for semantic composition to understand
  the meaning of text, a fundamental problem in natural language understanding, and
  show that it outperforms a state-of-the-art recursive model by replacing its composition
  layers with the S-LSTM memory blocks. We also show that utilizing the given structures
  is helpful in achieving a performance better than that without considering the structures.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhub15
month: 0
tex_title: Long Short-Term Memory Over Recursive Structures
firstpage: 1604
lastpage: 1612
page: 1604-1612
sections: 
author:
- given: Xiaodan
  family: Zhu
- given: Parinaz
  family: Sobihani
- given: Hongyu
  family: Guo
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/zhub15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
