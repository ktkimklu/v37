---
title: Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix
abstract: Second-order optimization methods, such as natural gradient, are difficult
  to apply to high-dimensional problems, because they require approximately solving
  large linear systems. We present FActorized Natural Gradient (FANG), an approximation
  to natural gradient descent where the Fisher matrix is approximated with a Gaussian
  graphical model whose precision matrix can be computed efficiently. We analyze the
  Fisher matrix for a small RBM and derive an extremely sparse graphical model which
  is a good match to the covariance of the sufficient statistics. Our experiments
  indicate that FANG allows RBMs to be trained more efficiently compared with stochastic
  gradient descent. Additionally, our analysis yields insight into the surprisingly
  good performance of the “centering trick” for training RBMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: grosse15
month: 0
tex_title: Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher
  Matrix
firstpage: 2304
lastpage: 2313
page: 2304-2313
order: 2304
cycles: false
author:
- given: Roger
  family: Grosse
- given: Ruslan
  family: Salakhudinov
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/grosse15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
