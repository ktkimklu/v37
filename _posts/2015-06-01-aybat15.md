---
supplementary: http://proceedings.mlr.press/v37/aybat15-supp.pdf
title: An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization
abstract: We propose a distributed first-order augmented Lagrangian (DFAL) algorithm
  to minimize the sum of composite convex functions, where each term in the sum is
  a private cost function belonging to a node, and only nodes connected by an edge
  can directly communicate with each other. This optimization model abstracts a number
  of applications in distributed sensing and machine learning. We show that any limit
  point of DFAL iterates is optimal; and for any eps > 0, an eps-optimal and eps-feasible
  solution can be computed within O(log(1/eps)) DFAL iterations, which require O(\psi_\textmax^1.5/d_\textmin
  ⋅1/ε) proximal gradient computations and communications per node in total, where
  \psi_\textmax denotes the largest eigenvalue of the graph Laplacian, and d_\textmin
  is the minimum degree of the graph. We also propose an asynchronous version of DFAL
  by incorporating randomized block coordinate descent methods; and demonstrate the
  efficiency of DFAL on large scale sparse-group LASSO problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: aybat15
month: 0
tex_title: An Asynchronous Distributed Proximal Gradient Method for Composite Convex
  Optimization
firstpage: 2454
lastpage: 2462
page: 2454-2462
order: 2454
cycles: false
author:
- given: Necdet
  family: Aybat
- given: Zi
  family: Wang
- given: Garud
  family: Iyengar
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/aybat15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
