---
title: An Empirical Exploration of Recurrent Network Architectures
abstract: The Recurrent Neural Network (RNN) is an extremely powerful sequence model
  that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific
  RNN architecture whose design makes it much easier to train. While wildly successful
  in practice, the LSTM’s architecture appears to be ad-hoc so it is not clear if
  it is optimal, and the significance of its individual components is unclear. In
  this work, we aim to determine whether the LSTM architecture is optimal or whether
  much better architectures exist. We conducted a thorough architecture search where
  we evaluated over ten thousand different RNN architectures, and identified an architecture
  that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit
  (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM’s
  forget gate closes the gap between the LSTM and the GRU.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jozefowicz15
month: 0
tex_title: An Empirical Exploration of Recurrent Network Architectures
firstpage: 2342
lastpage: 2350
page: 2342-2350
sections: 
author:
- given: Rafal
  family: Jozefowicz
- given: Wojciech
  family: Zaremba
- given: Ilya
  family: Sutskever
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/jozefowicz15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
