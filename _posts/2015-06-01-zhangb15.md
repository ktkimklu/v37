---
title: 'DiSCO: Distributed Optimization for Self-Concordant Empirical Loss'
abstract: 'We propose a new distributed algorithm for empirical risk minimization
  in machine learning. The algorithm is based on an inexact damped Newton method,
  where the inexact Newton steps are computed by a distributed preconditioned conjugate
  gradient method. We analyze its iteration complexity and communication efficiency
  for minimizing self-concordant empirical loss functions, and discuss the results
  for distributed ridge regression, logistic regression and binary classification
  with a smoothed hinge loss. In a standard setting for supervised learning, where
  the n data points are i.i.d. sampled and when the regularization parameter scales
  as 1/\sqrtn, we show that the proposed algorithm is communication efficient: the
  required round of communication does not increase with the sample size n, and only
  grows slowly with the number of machines.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhangb15
month: 0
tex_title: 'DiSCO: Distributed Optimization for Self-Concordant Empirical Loss'
firstpage: 362
lastpage: 370
page: 362-370
order: 362
cycles: false
author:
- given: Yuchen
  family: Zhang
- given: Xiao
  family: Lin
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/zhangb15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
