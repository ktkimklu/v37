---
title: Learning Submodular Losses with the Lovasz Hinge
abstract: Learning with non-modular losses is an important problem when sets of predictions
  are made simultaneously. The main tools for constructing convex surrogate loss functions
  for set prediction are margin rescaling and slack rescaling. In this work, we show
  that these strategies lead to tight convex surrogates iff the underlying loss function
  is increasing in the number of incorrect predictions. However, gradient or cutting-plane
  computation for these functions is NP-hard for non-supermodular loss functions.
  We propose instead a novel convex surrogate loss function for submodular losses,
  the Lovasz hinge, which leads to O(p log p) complexity with O(p) oracle accesses
  to the loss function to compute a gradient or cutting-plane. As a result, we have
  developed the first tractable convex surrogates in the literature for submodular
  losses. We demonstrate the utility of this novel convex surrogate through a real
  world image labeling task.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yub15
month: 0
tex_title: Learning Submodular Losses with the Lovasz Hinge
firstpage: 1623
lastpage: 1631
page: 1623-1631
sections: 
author:
- given: Jiaqian
  family: Yu
- given: Matthew
  family: Blaschko
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/yub15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
