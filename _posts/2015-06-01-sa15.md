---
supplementary: Supplementary:sa15-supp.pdf
title: Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix
  Problems
abstract: Stochastic gradient descent (SGD) on a low-rank factorization is commonly
  employed to speed up matrix problems including matrix completion, subspace tracking,
  and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank
  least-squares problem, and we prove that, under broad sampling conditions, our method
  converges globally from a random starting point within O(Îµ^-1 n \log n) steps with
  constant probability for constant-rank problems. Our modification of SGD relates
  it to stochastic power iteration. We also show some experiments to illustrate the
  runtime and convergence of the algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sa15
month: 0
firstpage: 2332
lastpage: 2341
page: 2332-2341
sections: 
author:
- given: Christopher De
  family: Sa
- given: Christopher
  family: Re
- given: Kunle
  family: Olukotun
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/sa15/sa15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
