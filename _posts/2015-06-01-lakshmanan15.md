---
title: Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning
abstract: We consider the problem of undiscounted reinforcement learning in continuous
  state space. Regret bounds in this setting usually hold under various assumptions
  on the structure of the reward and transition function. Under the assumption that
  the rewards and transition probabilities are Lipschitz, for 1-dimensional state
  space a regret bound of O(T^3/4) after any T steps has been given by Ortner and
  Ryabko (2012). Here we improve upon this result by using non-parametric kernel density
  estimation for estimating the transition probability distributions, and obtain regret
  bounds that depend on the smoothness of the transition probability distributions.
  In particular, under the assumption that the transition probability functions are
  smoothly differentiable, the regret bound is shown to be O(T^2/3) asymptotically
  for reinforcement learning in 1-dimensional state space. Finally, we also derive
  improved regret bounds for higher dimensional state space.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lakshmanan15
month: 0
tex_title: Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning
firstpage: 524
lastpage: 532
page: 524-532
order: 524
cycles: false
author:
- given: K.
  family: Lakshmanan
- given: Ronald
  family: Ortner
- given: Daniil
  family: Ryabko
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/lakshmanan15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
