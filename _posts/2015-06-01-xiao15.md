---
title: Is Feature Selection Secure against Training Data Poisoning?
abstract: Learning in adversarial settings is becoming an important task for application
  domains where attackers may inject malicious data into the training set to subvert
  normal operation of data-driven technologies. Feature selection has been widely
  used in machine learning for security applications to improve generalization and
  computational efficiency, although it is not clear whether its use may be beneficial
  or even counterproductive when training data are poisoned by intelligent attackers.
  In this work, we shed light on this issue by providing a framework to investigate
  the robustness of popular feature selection methods, including LASSO, ridge regression
  and the elastic net. Our results on malware detection show that feature selection
  methods can be significantly compromised under attack (we can reduce LASSO to almost
  random choices of feature sets by careful insertion of less than 5% poisoned training
  samples), highlighting the need for specific countermeasures.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: xiao15
month: 0
firstpage: 1689
lastpage: 1698
page: 1689-1698
sections: 
author:
- given: Huang
  family: Xiao
- given: Battista
  family: Biggio
- given: Gavin
  family: Brown
- given: Giorgio
  family: Fumera
- given: Claudia
  family: Eckert
- given: Fabio
  family: Roli
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/xiao15/xiao15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
