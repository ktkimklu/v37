---
supplementary: Supplementary:osogami15-supp.pdf
title: Robust partially observable Markov decision process
abstract: We seek to find the robust policy that maximizes the expected cumulative
  reward for the worst case when a partially observable Markov decision process (POMDP)
  has uncertain parameters whose values are only known to be in a given region. We
  prove that the robust value function, which represents the expected cumulative reward
  that can be obtained with the robust policy, is convex with respect to the belief
  state. Based on the convexity, we design a value-iteration algorithm for finding
  the robust policy. We prove that our value iteration converges for an infinite horizon.
  We also design point-based value iteration for fining the robust policy more efficiency
  possibly with approximation. Numerical experiments show that our point-based value
  iteration can adequately find robust policies.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: osogami15
month: 0
firstpage: 106
lastpage: 115
page: 106-115
sections: 
author:
- given: Takayuki
  family: Osogami
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/osogami15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
