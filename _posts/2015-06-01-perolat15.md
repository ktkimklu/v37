---
supplementary: http://proceedings.mlr.press/v37/perolat15-supp.pdf
title: Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games
abstract: This paper provides an analysis of error propagation in Approximate Dynamic
  Programming applied to zero-sum two-player Stochastic Games. We provide a novel
  and unified error propagation analysis in L_p-norm of three well-known algorithms
  adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy
  Iteration and Approximate Generalized Policy Iteration). We show that we can achieve
  a stationary policy which is \frac2γ(1 - γ)^2 ε+ \frac1(1 - γ)^2ε’-optimal, where
  εis the value function approximation error and ε’ is the approximate greedy operator
  error. In addition, we provide a practical algorithm (AGPI-Q) to solve infinite
  horizon γ-discounted two-player zero-sum stochastic games in a batch setting. It
  is an extension of the Fitted-Q algorithm (which solves Markov Decisions Processes
  in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally
  the performance of AGPI-Q on a simultaneous two-player game, namely Alesia.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: perolat15
month: 0
tex_title: Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games
firstpage: 1321
lastpage: 1329
page: 1321-1329
order: 1321
cycles: false
author:
- given: Julien
  family: Perolat
- given: Bruno
  family: Scherrer
- given: Bilal
  family: Piot
- given: Olivier
  family: Pietquin
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/perolat15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
