---
title: Stochastic Optimization with Importance Sampling for Regularized Loss Minimization
abstract: Uniform sampling of training data has been commonly used in traditional
  stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD)
  and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling
  can guarantee that the sampled stochastic quantity is an unbiased estimate of the
  corresponding true quantity, the resulting estimator may have a rather high variance,
  which negatively affects the convergence of the underlying optimization procedure.
  In this paper we study stochastic optimization, including prox-SMD and prox-SDCA,
  with importance sampling, which improves the convergence rate by reducing the stochastic
  variance. We theoretically analyze the algorithms and empirically validate their
  effectiveness.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhaoa15
month: 0
tex_title: Stochastic Optimization with Importance Sampling for Regularized Loss Minimization
firstpage: 1
lastpage: 9
page: 1-9
order: 1
cycles: false
author:
- given: Peilin
  family: Zhao
- given: Tong
  family: Zhang
date: 2015-06-01
address: Lille, France
publisher: PMLR
container-title: Proceedings of the 32nd International Conference on Machine Learning
volume: '37'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 1
pdf: http://proceedings.mlr.press/v37/zhaoa15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
