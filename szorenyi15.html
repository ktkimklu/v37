<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Qualitative Multi-Armed Bandits: A Quantile-Based Approach | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Qualitative Multi-Armed Bandits: A Quantile-Based Approach">

  <meta name="citation_author" content="Szorenyi, Balazs">

  <meta name="citation_author" content="Busa-Fekete, Robert">

  <meta name="citation_author" content="Weng, Paul">

  <meta name="citation_author" content="Hüllermeier, Eyke">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1660">
<meta name="citation_lastpage" content="1668">
<meta name="citation_pdf_url" content="szorenyi15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Qualitative Multi-Armed Bandits: A Quantile-Based Approach</h1>

	<div id="authors">
	
		Balazs Szorenyi,
	
		Robert Busa-Fekete,
	
		Paul Weng,
	
		Eyke Hüllermeier
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 1660–1668, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a finite (pure exploration) and infinite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of first experimental studies.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="szorenyi15.pdf">Download PDF</a></li>
			
			<li><a href="szorenyi15-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
