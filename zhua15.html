<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing">

  <meta name="citation_author" content="Zhu, Rongda">

  <meta name="citation_author" content="Gu, Quanquan">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="739">
<meta name="citation_lastpage" content="747">
<meta name="citation_pdf_url" content="zhua15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing</h1>

	<div id="authors">
	
		Rongda Zhu,
	
		Quanquan Gu
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 739â€“747, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		In this paper, we propose a novel algorithm based on nonconvex sparsity-inducing penalty for one-bit compressed sensing. We prove that our algorithm has a sample complexity of <span class="math">\(O(s/\epsilon^2)\)</span> for strong signals, and <span class="math">\(O(s\log d/\epsilon^2)\)</span> for weak signals, where <span class="math">\(s\)</span> is the number of nonzero entries in the signal vector, <span class="math">\(d\)</span> is the signal dimension and <span class="math">\(\epsilon\)</span> is the recovery error. For general signals, the sample complexity of our algorithm lies between <span class="math">\(O(s/\epsilon^2)\)</span> and <span class="math">\(O(s\log d/\epsilon^2)\)</span>. This is a remarkable improvement over the existing best sample complexity <span class="math">\(O(s\log d/\epsilon^2)\)</span>. Furthermore, we show that our algorithm achieves exact support recovery with high probability for strong signals. Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority of our algorithm for both approximate signal and support recovery in the noisy setting.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="zhua15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
