<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Cheap Bandits | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Cheap Bandits">

  <meta name="citation_author" content="Hanawal, Manjesh">

  <meta name="citation_author" content="Saligrama, Venkatesh">

  <meta name="citation_author" content="Valko, Michal">

  <meta name="citation_author" content="Munos, Remi">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="2133">
<meta name="citation_lastpage" content="2142">
<meta name="citation_pdf_url" content="hanawal15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Cheap Bandits</h1>

	<div id="authors">
	
		Manjesh Hanawal,
	
		Venkatesh Saligrama,
	
		Michal Valko,
	
		Remi Munos
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 2133â€“2142, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a <span class="math">\(\Omega(\sqrt(dT))\)</span> lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension <span class="math">\(d\)</span>.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="hanawal15.pdf">Download PDF</a></li>
			
			<li><a href="hanawal15-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
