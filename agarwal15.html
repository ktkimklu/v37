<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>A Lower Bound for the Optimization of Finite Sums | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="A Lower Bound for the Optimization of Finite Sums">

  <meta name="citation_author" content="Agarwal, Alekh">

  <meta name="citation_author" content="Bottou, Leon">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="78">
<meta name="citation_lastpage" content="86">
<meta name="citation_pdf_url" content="agarwal15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>A Lower Bound for the Optimization of Finite Sums</h1>

	<div id="authors">
	
		Alekh Agarwal,
	
		Leon Bottou
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 78â€“86, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		This paper presents a lower bound for optimizing a finite sum of <span class="math">\(n\)</span> functions, where each function is <span class="math">\(L\)</span>-smooth and the sum is <span class="math">\(\mu\)</span>-strongly convex. We show that no algorithm can reach an error <span class="math">\(\epsilon\)</span> in minimizing all functions from this class in fewer than <span class="math">\(\Omega(n + \sqrt{n(\kappa-1)}\log(1/\epsilon))\)</span> iterations, where <span class="math">\(\kappa=L/\mu\)</span> is a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="agarwal15.pdf">Download PDF</a></li>
			
			<li><a href="agarwal15-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
