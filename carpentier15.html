<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Simple regret for infinitely many armed bandits | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Simple regret for infinitely many armed bandits">

  <meta name="citation_author" content="Carpentier, Alexandra">

  <meta name="citation_author" content="Valko, Michal">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1133">
<meta name="citation_lastpage" content="1141">
<meta name="citation_pdf_url" content="carpentier15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Simple regret for infinitely many armed bandits</h1>

	<div id="authors">
	
		Alexandra Carpentier,
	
		Michal Valko
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 1133â€“1141, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter <span class="math">\(\beta\)</span> characterizing the distribution of the near-optimal arms. We prove that depending on <span class="math">\(\beta\)</span>, our algorithm is minimax optimal either up to a multiplicative constant or up to a <span class="math">\(\log(n)\)</span> factor. We also provide extensions to several important cases: when <span class="math">\(\beta\)</span> is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="carpentier15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
