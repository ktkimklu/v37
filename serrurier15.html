<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees">

  <meta name="citation_author" content="Serrurier, Mathieu">

  <meta name="citation_author" content="Prade, Henri">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1576">
<meta name="citation_lastpage" content="1584">
<meta name="citation_pdf_url" content="serrurier15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees</h1>

	<div id="authors">
	
		Mathieu Serrurier,
	
		Henri Prade
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 1576â€“1584, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Entropy gain is widely used for learning decision trees. However, as we go deeper downward the tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading choices and over-fitting may occur and the tree has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these methods still depends on the choices previously made, which may be unsatisfactory. We propose a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability distribution and the uncertainty around the estimation of its parameters. This function takes advantage of the ability of a possibility distribution to upper bound a family of probabilities previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure has several advantages over the classical one. It performs significant choices of split and provides a statistically relevant stopping criterion that allows the learning of trees whose size is well-suited w.r.t. the available data. On the top of that, it also provides a reasonable estimator of the performances of a decision tree. Finally, we show that it can be used for designing a simple and efficient online learning algorithm.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="serrurier15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
