<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Hashing for Distributed Data | ICML 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Hashing for Distributed Data">

  <meta name="citation_author" content="Leng, Cong">

  <meta name="citation_author" content="Wu, Jiaxiang">

  <meta name="citation_author" content="Cheng, Jian">

  <meta name="citation_author" content="Zhang, Xi">

  <meta name="citation_author" content="Lu, Hanqing">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 32nd International Conference on Machine Learning">
<meta name="citation_firstpage" content="1642">
<meta name="citation_lastpage" content="1650">
<meta name="citation_pdf_url" content="leng15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Hashing for Distributed Data</h1>

	<div id="authors">
	
		Cong Leng,
	
		Jiaxiang Wu,
	
		Jian Cheng,
	
		Xi Zhang,
	
		Hanqing Lu
	<br />
	</div>
	<div id="info">
		Proceedings of The 32nd International Conference on Machine Learning,
		pp. 1642â€“1650, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Recently, hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However, due to the large scale of many applications, the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper, we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="leng15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
